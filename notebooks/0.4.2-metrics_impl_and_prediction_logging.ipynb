{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wKvV6g4CZ9Ut"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/coder/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install \"transformers[torch]\" datasets -q\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BHpLhkagbLpv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/coder/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate rouge_score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "edRBt2mgZ9Ux"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-icu_HPwZ9Uy"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WXj_5feJxYnt"
      },
      "outputs": [],
      "source": [
        "!pip install \"transformers[torch]\" sentencepiece datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cdt7Yi_yX3eK"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2FwZ_3xnxYnr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ogW79RNbxYns"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# data_path = \"data/processed/dialogsum_google_to_rus/\"\n",
        "# data_path = \".\"\n",
        "data_path = \"..\"\n",
        "\n",
        "data_val = pd.read_csv(os.path.join(data_path, \"output_validation.csv\"))\n",
        "data_train = pd.read_csv(os.path.join(data_path, \"output_train.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrLDwurgFixh",
        "outputId": "e6a5a410-038e-4ae6-d617-50964bf70b14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'dialogue': '#–ß–µ–ª–æ–≤–µ–∫1#: –ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã, —á—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∏–≤–∞ –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ –ø–µ—Ç—å?\\n#–ß–µ–ª–æ–≤–µ–∫2#: –¢—ã —É–≤–µ—Ä–µ–Ω? –û—Ç–∫—É–¥–∞ –≤—ã –∑–Ω–∞–µ—Ç–µ?\\n#–ß–µ–ª–æ–≤–µ–∫1#: –ù—É, –æ–±—ã—á–Ω–æ –ª—é–¥–∏ –¥—É–º–∞—é—Ç, —á—Ç–æ —è —É–∂–∞—Å–Ω—ã–π –ø–µ–≤–µ—Ü, –Ω–æ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –≤—Å–µ –≤—ã–ø—å–µ–º –Ω–µ–º–Ω–æ–≥–æ –ø–∏–≤–∞, –æ–Ω–∏ –≥–æ–≤–æ—Ä—è—Ç, —á—Ç–æ —è –∑–≤—É—á—É –Ω–∞–º–Ω–æ–≥–æ –ª—É—á—à–µ!\\n#–ß–µ–ª–æ–≤–µ–∫2#: –ù—É, —è —Å–ª—ã—à–∞–ª, —á—Ç–æ –µ—Å–ª–∏ –ø–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∏–≤–∞, —Ç–æ —Å–º–æ–∂–µ—à—å –ª—É—á—à–µ –≥–æ–≤–æ—Ä–∏—Ç—å –Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. . .\\n#–ß–µ–ª–æ–≤–µ–∫1#: –¢–æ–≥–¥–∞, –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫—Ä—É–∂–µ–∫ –ø–∏–≤–∞, —Ç—ã –±—É–¥–µ—à—å –ø–µ—Ç—å –Ω–∞ —Ç–∞–π–≤–∞–Ω—å—Å–∫–æ–º?\\n#–ß–µ–ª–æ–≤–µ–∫2#: –í–æ–∑–º–æ–∂–Ω–æ. . .',\n",
              " 'summary': '#–ß–µ–ª–æ–≤–µ–∫1# –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –ø–∏–≤–æ –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ –ø–µ—Ç—å, –Ω–æ #–ß–µ–ª–æ–≤–µ–∫2# —Å–ª—ã—à–∞–ª, —á—Ç–æ –ø–∏–≤–æ –ø–æ–º–æ–≥–∞–µ—Ç –≥–æ–≤–æ—Ä–∏—Ç—å –Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datasets\n",
        "from datasets import DatasetDict\n",
        "def make_dataset(dataframe):\n",
        "    data = dataframe[[\"dialogue\", \"summary\"]]\n",
        "    # data.columns = [\"input_ids\", \"labels\"]\n",
        "    dataset = datasets.Dataset.from_pandas(data)\n",
        "    return dataset\n",
        "dataset_train = make_dataset(data_train)\n",
        "dataset_val = make_dataset(data_val)\n",
        "dataset_dict = DatasetDict({\n",
        "  \"train\": dataset_train,\n",
        "  \"validation\": dataset_val,\n",
        "})\n",
        "dataset_val[20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkf0dfpXZ9Uy"
      },
      "source": [
        "Start by loading the smaller California state bill subset of the BillSum dataset from the ü§ó Datasets library:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTUfAzXQZ9Uz"
      },
      "source": [
        "There are two fields that you'll want to use:\n",
        "\n",
        "- `dialogue`: the text of the bill which'll be the input to the model.\n",
        "- `summary`: a condensed version of `text` which'll be the model target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ksyIYKzgTUg"
      },
      "source": [
        "## Model and tokenizer loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SIy1DEwMZ9U0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [00:00<00:00, 26.4kB/s]\n",
            "Downloading spiece.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 828k/828k [00:00<00:00, 2.42MB/s]\n",
            "Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.31M/1.31M [00:00<00:00, 53.8MB/s]\n",
            "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65.0/65.0 [00:00<00:00, 25.9kB/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"IlyaGusev/rut5_base_headline_gen_telegram\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z6Ud31alh11Q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 766/766 [00:00<00:00, 64.8kB/s]\n",
            "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 977M/977M [00:18<00:00, 53.8MB/s] \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyFg892e7d4k"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T_LLH5Tt7d4l"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "validation_data = dataset_dict[\"validation\"].select(range(n_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N-InxMDC7d4l"
      },
      "outputs": [],
      "source": [
        "text = validation_data[0][\"dialogue\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOrHRY6o7d4l",
        "outputId": "25a106a2-f7c6-4005-f9bd-029d1b247bb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': '#–ß–µ–ª–æ–≤–µ–∫2#: –£ –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º—ã —Å –¥—ã—Ö–∞–Ω–∏–µ–º, —è –Ω–µ –ø—Ä–æ—Å—Ç—É–¥–∏–ª—Å—è'}]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "summarizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfmGVfe57d4l"
      },
      "source": [
        "Log the results to W&B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sxoYY6XT7d4m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/coder/.netrc\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/coder/project/notebooks/wandb/run-20231114_231952-iqu1iubl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/amirfvb/project-notebooks/runs/iqu1iubl' target=\"_blank\">divine-water-1</a></strong> to <a href='https://wandb.ai/amirfvb/project-notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/amirfvb/project-notebooks' target=\"_blank\">https://wandb.ai/amirfvb/project-notebooks</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/amirfvb/project-notebooks/runs/iqu1iubl' target=\"_blank\">https://wandb.ai/amirfvb/project-notebooks/runs/iqu1iubl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/amirfvb/project-notebooks/runs/iqu1iubl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f349515ed90>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6rFFEUe7d4m",
        "outputId": "4ca35552-5b6d-464a-ca82-3dea635e930a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/coder/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 150, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "table = wandb.Table(columns=[\"Input Text\", \"Target Summary\", \"Generated Summary\"])\n",
        "\n",
        "# Process each example in the validation dataset and append to the table_data list\n",
        "for example in validation_data:\n",
        "    input_text = example[\"dialogue\"]  # Replace with the actual key in your dataset\n",
        "    target_summary = example[\"summary\"]  # Replace with the actual key in your dataset\n",
        "\n",
        "    # Generate summary using the pipeline\n",
        "    generated_summary = summarizer(input_text, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Append row to table_data\n",
        "    table.add_data(input_text, target_summary, generated_summary[0][\"summary_text\"])\n",
        "\n",
        "# Create a WandB Table and log it\n",
        "wandb.log({\"summarization_before_fine_tuning\": table})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lkziU5wZ9Uz"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fb33P6eZ9U0"
      },
      "source": [
        "The preprocessing function you want to create needs to:\n",
        "\n",
        "1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
        "2. Use the keyword `text_target` argument when tokenizing labels.\n",
        "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jiQrww3YZ9U0"
      },
      "outputs": [],
      "source": [
        "prefix = \"\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJUTvMBBZ9U0"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ddb7567b252d418c84fd6040e7cb25a8",
            "8ff207ccd9e1484392ce127a63a2b4e7",
            "da66efcd6ed54ad5907c681515a16914",
            "1c52ce0e552242bbaf247ffdb89b4b3d",
            "40a40fcfe326431d9d0b9cc5c5965f09",
            "bfda6ef5da74470892a925dce1d89986",
            "ce3d5bca46a1434d84e3ba0c3e88fc12",
            "d2b736b2c997482b86bd92d737a3c424",
            "a0f2bd31c0fb441ba36e648107aff3b2",
            "dd753e1076db44528e982f7da838c202",
            "608c5998c55748f5b30b4416168ba7b3",
            "ed8ef53e0fac487589268fe5fea00361",
            "471d53342b1d4001839acb02ec7f2013",
            "ae0570496d8f44fd9ac9aa3a096c1cea",
            "4a43966de75d4473ad3069797473b402",
            "c46aba7038db4e17902a7d0891030eb1",
            "c2c8389e0a60466e96b5f41b608f7399",
            "6cc7a436bbd64cc48c8953a1f456d3ac",
            "3156d3d9f58a4f15965855590642be57",
            "dbc54db6f1ab4a0e839bde9c1d5fb5d5",
            "72974cf3701841df830072f6a57ad4b3",
            "30ad468f29304da59c6131601ead5182"
          ]
        },
        "id": "oAfa47Z2Z9U0",
        "outputId": "f81e8f43-2130-4624-aed2-55f26fd6e94f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12460/12460 [00:03<00:00, 3257.22 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 3123.68 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9Ht6d_Z9U0"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-qFCUs3aZ9U0"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAS95s-QZ9U1"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXt1Mc2ZZ9U1"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TwUI6N1cZ9U1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.27k/6.27k [00:00<00:00, 4.24MB/s]\n",
            "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.94k/5.94k [00:00<00:00, 4.47MB/s]\n",
            "Downloading extra modules: 4.07kB [00:00, 3.91MB/s]                   \n",
            "Downloading extra modules: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.34k/3.34k [00:00<00:00, 3.11MB/s]\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hzKDKUZ9U1"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the ROUGE metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VhizjayeZ9U1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result_rouge = rouge.compute(predictions=decoded_preds,\n",
        "                        references=decoded_labels,\n",
        "                        tokenizer=tokenizer.tokenize)\n",
        "\n",
        "    bleu_results = bleu.compute(predictions=decoded_preds, references=decoded_labels, tokenizer=tokenizer.tokenize)\n",
        "    result = {\n",
        "        **{\"rouge_\" + k: v for k, v in result_rouge.items()},\n",
        "        **{\"bleu_\" + k: v for k, v in bleu_results.items()},\n",
        "    }\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "\n",
        "    return {k: round(v, 4) if isinstance(v, float) else v\n",
        "            for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILGUKPezZ9U1"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VSdmFxqZ9U1"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgZ_wIO7Z9U1"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "a_J6_DSsZ9U1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUKTsTpZ9U1"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the ROUGE metric and save the training checkpoint.\n",
        "2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kV4RoJYxZ9U1",
        "outputId": "c0ff3cbd-44a3-4a9a-be1e-cbe82a258070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1344' max='41540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1344/41540 09:07 < 4:33:08, 2.45 it/s, Epoch 0.65/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m training_args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy_awesome_billsum_model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/notebooks/0.4.2-metrics_impl_and_prediction_logging.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2736\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:401\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    394\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    395\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    400\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 401\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:191\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    188\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    193\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"tg-summarizer\" # name your W&B project\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_awesome_billsum_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=3,\n",
        "    per_device_eval_batch_size=3,\n",
        "    logging_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=20,\n",
        "    predict_with_generate=True,\n",
        "    # fp16=use_fp16,\n",
        "    # bf16=True,\n",
        "    generation_max_length=100,\n",
        "    # push_to_hub=True,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"], #.select(range(1000)), # out of 12k\n",
        "    eval_dataset=tokenized_dataset[\"validation\"].select(range(100)),\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnzhWpjPZ9U2"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P32EzyogZ9U2"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEidf6FlZ9U2"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for summarization, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB4mdK-9Z9U2"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z11zm711P6g"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "validation_data = dataset_dict[\"validation\"].select(range(n_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO34tZ483B2c"
      },
      "outputs": [],
      "source": [
        "text = validation_data[0][\"dialogue\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68stLuAYZ9U3",
        "outputId": "24e8443a-b2f7-48fe-ae6f-842ebd4677df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': '#–ß–µ–ª–æ–≤–µ–∫2# –∂–∞–ª—É–µ—Ç—Å—è, —á—Ç–æ —É –Ω–µ–≥–æ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥—ã—Ö–∞–Ω–∏–µ–º. #Person1# —Å–æ–æ–±—â–∞–µ—Ç #Person1#, —á—Ç–æ —É #Person1# –Ω–µ—Ç –∞–ª–ª–µ—Ä–≥–∏–∏.'}]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "summarizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58d8ySn0MGE"
      },
      "source": [
        "Log the results to W&B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rclymDF0Nto"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89NMkT2q1MMQ",
        "outputId": "658b6734-ecec-4ff7-86a4-c3cd60c17ecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 150, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "table = wandb.Table(columns=[\"Input Text\", \"Target Summary\", \"Generated Summary\"])\n",
        "\n",
        "# Process each example in the validation dataset and append to the table_data list\n",
        "for example in validation_data:\n",
        "    input_text = example[\"dialogue\"]  # Replace with the actual key in your dataset\n",
        "    target_summary = example[\"summary\"]  # Replace with the actual key in your dataset\n",
        "\n",
        "    # Generate summary using the pipeline\n",
        "    generated_summary = summarizer(input_text, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Append row to table_data\n",
        "    table.add_data(input_text, target_summary, generated_summary[0][\"summary_text\"])\n",
        "\n",
        "# Create a WandB Table and log it\n",
        "wandb.log({\"summarization_after_fine_tuning\": table})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c52ce0e552242bbaf247ffdb89b4b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd753e1076db44528e982f7da838c202",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_608c5998c55748f5b30b4416168ba7b3",
            "value": " 12460/12460 [00:12&lt;00:00, 806.28 examples/s]"
          }
        },
        "30ad468f29304da59c6131601ead5182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3156d3d9f58a4f15965855590642be57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a40fcfe326431d9d0b9cc5c5965f09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471d53342b1d4001839acb02ec7f2013": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2c8389e0a60466e96b5f41b608f7399",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6cc7a436bbd64cc48c8953a1f456d3ac",
            "value": "Map: 100%"
          }
        },
        "4a43966de75d4473ad3069797473b402": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72974cf3701841df830072f6a57ad4b3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30ad468f29304da59c6131601ead5182",
            "value": " 500/500 [00:00&lt;00:00, 717.82 examples/s]"
          }
        },
        "608c5998c55748f5b30b4416168ba7b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cc7a436bbd64cc48c8953a1f456d3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72974cf3701841df830072f6a57ad4b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff207ccd9e1484392ce127a63a2b4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfda6ef5da74470892a925dce1d89986",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ce3d5bca46a1434d84e3ba0c3e88fc12",
            "value": "Map: 100%"
          }
        },
        "a0f2bd31c0fb441ba36e648107aff3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae0570496d8f44fd9ac9aa3a096c1cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3156d3d9f58a4f15965855590642be57",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbc54db6f1ab4a0e839bde9c1d5fb5d5",
            "value": 500
          }
        },
        "bfda6ef5da74470892a925dce1d89986": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c8389e0a60466e96b5f41b608f7399": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46aba7038db4e17902a7d0891030eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3d5bca46a1434d84e3ba0c3e88fc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2b736b2c997482b86bd92d737a3c424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da66efcd6ed54ad5907c681515a16914": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2b736b2c997482b86bd92d737a3c424",
            "max": 12460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0f2bd31c0fb441ba36e648107aff3b2",
            "value": 12460
          }
        },
        "dbc54db6f1ab4a0e839bde9c1d5fb5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd753e1076db44528e982f7da838c202": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddb7567b252d418c84fd6040e7cb25a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ff207ccd9e1484392ce127a63a2b4e7",
              "IPY_MODEL_da66efcd6ed54ad5907c681515a16914",
              "IPY_MODEL_1c52ce0e552242bbaf247ffdb89b4b3d"
            ],
            "layout": "IPY_MODEL_40a40fcfe326431d9d0b9cc5c5965f09"
          }
        },
        "ed8ef53e0fac487589268fe5fea00361": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_471d53342b1d4001839acb02ec7f2013",
              "IPY_MODEL_ae0570496d8f44fd9ac9aa3a096c1cea",
              "IPY_MODEL_4a43966de75d4473ad3069797473b402"
            ],
            "layout": "IPY_MODEL_c46aba7038db4e17902a7d0891030eb1"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
