{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wKvV6g4CZ9Ut"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers[torch] datasets -q\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BHpLhkagbLpv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate rouge_score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "edRBt2mgZ9Ux"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-icu_HPwZ9Uy"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WXj_5feJxYnt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] sentencepiece datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "cdt7Yi_yX3eK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2FwZ_3xnxYnr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ogW79RNbxYns"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# data_path = \"data/processed/dialogsum_google_to_rus/\"\n",
        "data_path = \".\"\n",
        "\n",
        "data_val = pd.read_csv(os.path.join(data_path, \"output_validation.csv\"))\n",
        "data_train = pd.read_csv(os.path.join(data_path, \"output_train.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrLDwurgFixh",
        "outputId": "e6a5a410-038e-4ae6-d617-50964bf70b14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dialogue': '#–ß–µ–ª–æ–≤–µ–∫1#: –ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã, —á—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∏–≤–∞ –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ –ø–µ—Ç—å?\\n#–ß–µ–ª–æ–≤–µ–∫2#: –¢—ã —É–≤–µ—Ä–µ–Ω? –û—Ç–∫—É–¥–∞ –≤—ã –∑–Ω–∞–µ—Ç–µ?\\n#–ß–µ–ª–æ–≤–µ–∫1#: –ù—É, –æ–±—ã—á–Ω–æ –ª—é–¥–∏ –¥—É–º–∞—é—Ç, —á—Ç–æ —è —É–∂–∞—Å–Ω—ã–π –ø–µ–≤–µ—Ü, –Ω–æ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –≤—Å–µ –≤—ã–ø—å–µ–º –Ω–µ–º–Ω–æ–≥–æ –ø–∏–≤–∞, –æ–Ω–∏ –≥–æ–≤–æ—Ä—è—Ç, —á—Ç–æ —è –∑–≤—É—á—É –Ω–∞–º–Ω–æ–≥–æ –ª—É—á—à–µ!\\n#–ß–µ–ª–æ–≤–µ–∫2#: –ù—É, —è —Å–ª—ã—à–∞–ª, —á—Ç–æ –µ—Å–ª–∏ –ø–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∏–≤–∞, —Ç–æ —Å–º–æ–∂–µ—à—å –ª—É—á—à–µ –≥–æ–≤–æ—Ä–∏—Ç—å –Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. . .\\n#–ß–µ–ª–æ–≤–µ–∫1#: –¢–æ–≥–¥–∞, –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫—Ä—É–∂–µ–∫ –ø–∏–≤–∞, —Ç—ã –±—É–¥–µ—à—å –ø–µ—Ç—å –Ω–∞ —Ç–∞–π–≤–∞–Ω—å—Å–∫–æ–º?\\n#–ß–µ–ª–æ–≤–µ–∫2#: –í–æ–∑–º–æ–∂–Ω–æ. . .',\n",
              " 'summary': '#–ß–µ–ª–æ–≤–µ–∫1# –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –ø–∏–≤–æ –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ –ø–µ—Ç—å, –Ω–æ #–ß–µ–ª–æ–≤–µ–∫2# —Å–ª—ã—à–∞–ª, —á—Ç–æ –ø–∏–≤–æ –ø–æ–º–æ–≥–∞–µ—Ç –≥–æ–≤–æ—Ä–∏—Ç—å –Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import datasets\n",
        "from datasets import DatasetDict\n",
        "def make_dataset(dataframe):\n",
        "    data = dataframe[[\"dialogue\", \"summary\"]]\n",
        "    # data.columns = [\"input_ids\", \"labels\"]\n",
        "    dataset = datasets.Dataset.from_pandas(data)\n",
        "    return dataset\n",
        "dataset_train = make_dataset(data_train)\n",
        "dataset_val = make_dataset(data_val)\n",
        "dataset_dict = DatasetDict({\n",
        "  \"train\": dataset_train,\n",
        "  \"validation\": dataset_val,\n",
        "})\n",
        "dataset_val[20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkf0dfpXZ9Uy"
      },
      "source": [
        "Start by loading the smaller California state bill subset of the BillSum dataset from the ü§ó Datasets library:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTUfAzXQZ9Uz"
      },
      "source": [
        "There are two fields that you'll want to use:\n",
        "\n",
        "- `dialogue`: the text of the bill which'll be the input to the model.\n",
        "- `summary`: a condensed version of `text` which'll be the model target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ksyIYKzgTUg"
      },
      "source": [
        "## Model and tokenizer loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SIy1DEwMZ9U0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"IlyaGusev/rut5_base_headline_gen_telegram\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Z6Ud31alh11Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyFg892e7d4k"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 10\n",
        "validation_data = dataset_dict[\"validation\"].select(range(n_samples))"
      ],
      "metadata": {
        "id": "T_LLH5Tt7d4l"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = validation_data[0][\"dialogue\"]"
      ],
      "metadata": {
        "id": "N-InxMDC7d4l"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a106a2-f7c6-4005-f9bd-029d1b247bb6",
        "id": "dOrHRY6o7d4l"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': '#–ß–µ–ª–æ–≤–µ–∫2#: –£ –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º—ã —Å –¥—ã—Ö–∞–Ω–∏–µ–º, —è –Ω–µ –ø—Ä–æ—Å—Ç—É–¥–∏–ª—Å—è'}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "summarizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log the results to W&B:"
      ],
      "metadata": {
        "id": "WfmGVfe57d4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "sxoYY6XT7d4m"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = wandb.Table(columns=[\"Input Text\", \"Target Summary\", \"Generated Summary\"])\n",
        "\n",
        "# Process each example in the validation dataset and append to the table_data list\n",
        "for example in validation_data:\n",
        "    input_text = example[\"dialogue\"]  # Replace with the actual key in your dataset\n",
        "    target_summary = example[\"summary\"]  # Replace with the actual key in your dataset\n",
        "\n",
        "    # Generate summary using the pipeline\n",
        "    generated_summary = summarizer(input_text, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Append row to table_data\n",
        "    table.add_data(input_text, target_summary, generated_summary[0][\"summary_text\"])\n",
        "\n",
        "# Create a WandB Table and log it\n",
        "wandb.log({\"summarization_before_fine_tuning\": table})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca35552-5b6d-464a-ca82-3dea635e930a",
        "id": "B6rFFEUe7d4m"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 150, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpoyQ-EobSG6"
      },
      "source": [
        "## Different metrics test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9FYrDPokkr67"
      },
      "outputs": [],
      "source": [
        "import evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1Qk-wt1abe3C"
      },
      "outputs": [],
      "source": [
        "# test text pieces\n",
        "predictions = [\"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\", \"–ø—Ä–∏–≤–µ—Ç –±—Ä–∞—Ç—å—è\", \"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\"]\n",
        "references = [[\" –Ω–∞ —Ä—É—Å—Å–∫–æ–π –º–æ–ª–≤–µ\", \"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\"], [\"–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Å—Ç–µ –∫–æ–º—Ä–∞–¥—ã\"], [\"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\"]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSu_TOBNlJGT",
        "outputId": "825ab882-6ea1-4312-d5bd-2e3536c24ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.9099882808096075, 'precisions': [0.8, 0.8571428571428571, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1111111111111112, 'translation_length': 10, 'reference_length': 9}\n"
          ]
        }
      ],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uygcTHEjeh1Z",
        "outputId": "d827f49b-4758-4579-d578-edfcb4e62f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.9099882808096075, 'precisions': [0.8, 0.8571428571428571, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1111111111111112, 'translation_length': 10, 'reference_length': 9}\n"
          ]
        }
      ],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions, references=references, tokenizer=lambda x: x.split())\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htcNM1BFido9",
        "outputId": "246b1fc1-4bf7-47fa-dadf-dcfb25ed83ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n"
          ]
        }
      ],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions, references=references, tokenizer=tokenizer)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB9cs-FfiqB5",
        "outputId": "695a9829-7014-42a2-a509-00ba6487596d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.7929873728856548, 'precisions': [0.8333333333333334, 0.8, 0.8333333333333334, 0.8888888888888888], 'brevity_penalty': 0.9459594689067654, 'length_ratio': 0.9473684210526315, 'translation_length': 18, 'reference_length': 19}\n"
          ]
        }
      ],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions, references=references, tokenizer=tokenizer.tokenize)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR9nz42mis0o",
        "outputId": "f969562b-aedb-4078-d323-b1cae052fb81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [259, 16735, 4217, 401, 259, 9807, 13796, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "['‚ñÅ', '—Ä—É—Å—Å–∫–∏–π', '‚ñÅ—è–∑—ã–∫', '‚ñÅ–Ω–µ', '‚ñÅ', '–æ–∂–∏–¥–∞', '–Ω–Ω–æ']\n",
            "['—Ä—É—Å—Å–∫–∏–π', '—è–∑—ã–∫', '–Ω–µ', '–æ–∂–∏–¥–∞–Ω–Ω–æ']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer(\"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\"))\n",
        "print(tokenizer.tokenize(\"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\"))\n",
        "print(\"—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ –æ–∂–∏–¥–∞–Ω–Ω–æ\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLmqVthalZ8P",
        "outputId": "4b3a5da2-a58a-45a5-db54-dc1f77aaf135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5VAUKLkhkes",
        "outputId": "cd372e57-debf-44bc-e734-66e35822ded2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.6666666666666666, 'rouge2': 0.6666666666666666, 'rougeL': 0.6666666666666666, 'rougeLsum': 0.6666666666666666}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references,\n",
        "                        tokenizer=lambda x: x.split())\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA8v6htMiVlb",
        "outputId": "ab6e8218-c6e5-48cb-85dc-fe0442dce7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.7272727272727272, 'rouge2': 0.6666666666666666, 'rougeL': 0.7272727272727272, 'rougeLsum': 0.7272727272727272}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references,\n",
        "                        tokenizer=tokenizer.tokenize)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s3sPMYalm-p"
      },
      "source": [
        "–í –æ–±—â–µ–º –¥—É–º–∞—é –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä —É–º–Ω—ã–π, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —É—á–∏—Ç—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ—Ä–µ–Ω—å —Ö–æ—Ç—è –±—ã –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –æ–∫–æ–Ω—á–∞–Ω–∏—è—Ö."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lkziU5wZ9Uz"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fb33P6eZ9U0"
      },
      "source": [
        "The preprocessing function you want to create needs to:\n",
        "\n",
        "1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
        "2. Use the keyword `text_target` argument when tokenizing labels.\n",
        "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jiQrww3YZ9U0"
      },
      "outputs": [],
      "source": [
        "prefix = \"\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJUTvMBBZ9U0"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ddb7567b252d418c84fd6040e7cb25a8",
            "8ff207ccd9e1484392ce127a63a2b4e7",
            "da66efcd6ed54ad5907c681515a16914",
            "1c52ce0e552242bbaf247ffdb89b4b3d",
            "40a40fcfe326431d9d0b9cc5c5965f09",
            "bfda6ef5da74470892a925dce1d89986",
            "ce3d5bca46a1434d84e3ba0c3e88fc12",
            "d2b736b2c997482b86bd92d737a3c424",
            "a0f2bd31c0fb441ba36e648107aff3b2",
            "dd753e1076db44528e982f7da838c202",
            "608c5998c55748f5b30b4416168ba7b3",
            "ed8ef53e0fac487589268fe5fea00361",
            "471d53342b1d4001839acb02ec7f2013",
            "ae0570496d8f44fd9ac9aa3a096c1cea",
            "4a43966de75d4473ad3069797473b402",
            "c46aba7038db4e17902a7d0891030eb1",
            "c2c8389e0a60466e96b5f41b608f7399",
            "6cc7a436bbd64cc48c8953a1f456d3ac",
            "3156d3d9f58a4f15965855590642be57",
            "dbc54db6f1ab4a0e839bde9c1d5fb5d5",
            "72974cf3701841df830072f6a57ad4b3",
            "30ad468f29304da59c6131601ead5182"
          ]
        },
        "id": "oAfa47Z2Z9U0",
        "outputId": "f81e8f43-2130-4624-aed2-55f26fd6e94f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddb7567b252d418c84fd6040e7cb25a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed8ef53e0fac487589268fe5fea00361"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9Ht6d_Z9U0"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-qFCUs3aZ9U0"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAS95s-QZ9U1"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXt1Mc2ZZ9U1"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TwUI6N1cZ9U1"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hzKDKUZ9U1"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the ROUGE metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VhizjayeZ9U1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result_rouge = rouge.compute(predictions=decoded_preds,\n",
        "                        references=decoded_labels,\n",
        "                        tokenizer=tokenizer.tokenize)\n",
        "\n",
        "    bleu_results = bleu.compute(predictions=decoded_preds, references=decoded_labels, tokenizer=tokenizer.tokenize)\n",
        "    result = {\n",
        "        **{\"rouge_\" + k: v for k, v in result_rouge.items()},\n",
        "        **{\"bleu_\" + k: v for k, v in bleu_results.items()},\n",
        "    }\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "\n",
        "    return {k: round(v, 4) if isinstance(v, float) else v\n",
        "            for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILGUKPezZ9U1"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VSdmFxqZ9U1"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgZ_wIO7Z9U1"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a_J6_DSsZ9U1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUKTsTpZ9U1"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the ROUGE metric and save the training checkpoint.\n",
        "2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.login()"
      ],
      "metadata": {
        "id": "TqeQUayaYFOl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kV4RoJYxZ9U1",
        "outputId": "c0ff3cbd-44a3-4a9a-be1e-cbe82a258070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirfvb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231114_213302-bigp63s8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/amirfvb/tg-summarizer/runs/bigp63s8' target=\"_blank\">crisp-meadow-9</a></strong> to <a href='https://wandb.ai/amirfvb/tg-summarizer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/amirfvb/tg-summarizer' target=\"_blank\">https://wandb.ai/amirfvb/tg-summarizer</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/amirfvb/tg-summarizer/runs/bigp63s8' target=\"_blank\">https://wandb.ai/amirfvb/tg-summarizer/runs/bigp63s8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3223' max='6680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3223/6680 28:49 < 30:56, 1.86 it/s, Epoch 9.65/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge Rouge1</th>\n",
              "      <th>Rouge Rouge2</th>\n",
              "      <th>Rouge Rougel</th>\n",
              "      <th>Rouge Rougelsum</th>\n",
              "      <th>Bleu Bleu</th>\n",
              "      <th>Bleu Precisions</th>\n",
              "      <th>Bleu Brevity Penalty</th>\n",
              "      <th>Bleu Length Ratio</th>\n",
              "      <th>Bleu Translation Length</th>\n",
              "      <th>Bleu Reference Length</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.691200</td>\n",
              "      <td>1.386762</td>\n",
              "      <td>0.441400</td>\n",
              "      <td>0.239300</td>\n",
              "      <td>0.376600</td>\n",
              "      <td>0.377200</td>\n",
              "      <td>0.241600</td>\n",
              "      <td>[0.4792626728110599, 0.28537735849056606, 0.1956521739130435, 0.14356435643564355]</td>\n",
              "      <td>0.970500</td>\n",
              "      <td>0.970900</td>\n",
              "      <td>434</td>\n",
              "      <td>447</td>\n",
              "      <td>45.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.560200</td>\n",
              "      <td>1.304369</td>\n",
              "      <td>0.467400</td>\n",
              "      <td>0.278400</td>\n",
              "      <td>0.400500</td>\n",
              "      <td>0.400900</td>\n",
              "      <td>0.256600</td>\n",
              "      <td>[0.555256064690027, 0.3518005540166205, 0.2564102564102564, 0.19648093841642228]</td>\n",
              "      <td>0.814800</td>\n",
              "      <td>0.830000</td>\n",
              "      <td>371</td>\n",
              "      <td>447</td>\n",
              "      <td>39.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.491000</td>\n",
              "      <td>1.281436</td>\n",
              "      <td>0.468200</td>\n",
              "      <td>0.282500</td>\n",
              "      <td>0.401600</td>\n",
              "      <td>0.400400</td>\n",
              "      <td>0.278800</td>\n",
              "      <td>[0.4831223628691983, 0.30603448275862066, 0.22687224669603523, 0.18018018018018017]</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.060400</td>\n",
              "      <td>474</td>\n",
              "      <td>447</td>\n",
              "      <td>49.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.421700</td>\n",
              "      <td>1.240335</td>\n",
              "      <td>0.491500</td>\n",
              "      <td>0.301600</td>\n",
              "      <td>0.417700</td>\n",
              "      <td>0.418400</td>\n",
              "      <td>0.295600</td>\n",
              "      <td>[0.508695652173913, 0.3288888888888889, 0.24545454545454545, 0.18604651162790697]</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.029100</td>\n",
              "      <td>460</td>\n",
              "      <td>447</td>\n",
              "      <td>48.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.332800</td>\n",
              "      <td>1.246128</td>\n",
              "      <td>0.464600</td>\n",
              "      <td>0.263500</td>\n",
              "      <td>0.396600</td>\n",
              "      <td>0.396600</td>\n",
              "      <td>0.262800</td>\n",
              "      <td>[0.5023255813953489, 0.30714285714285716, 0.21951219512195122, 0.165]</td>\n",
              "      <td>0.961200</td>\n",
              "      <td>0.962000</td>\n",
              "      <td>430</td>\n",
              "      <td>447</td>\n",
              "      <td>45.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.310400</td>\n",
              "      <td>1.206764</td>\n",
              "      <td>0.467300</td>\n",
              "      <td>0.276500</td>\n",
              "      <td>0.400700</td>\n",
              "      <td>0.402500</td>\n",
              "      <td>0.274300</td>\n",
              "      <td>[0.5058548009367682, 0.31894484412470026, 0.2334152334152334, 0.181360201511335]</td>\n",
              "      <td>0.954200</td>\n",
              "      <td>0.955300</td>\n",
              "      <td>427</td>\n",
              "      <td>447</td>\n",
              "      <td>44.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.294000</td>\n",
              "      <td>1.215250</td>\n",
              "      <td>0.472000</td>\n",
              "      <td>0.283500</td>\n",
              "      <td>0.416000</td>\n",
              "      <td>0.416800</td>\n",
              "      <td>0.284700</td>\n",
              "      <td>[0.5, 0.3165137614678899, 0.2323943661971831, 0.18028846153846154]</td>\n",
              "      <td>0.997800</td>\n",
              "      <td>0.997800</td>\n",
              "      <td>446</td>\n",
              "      <td>447</td>\n",
              "      <td>46.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.244600</td>\n",
              "      <td>1.200177</td>\n",
              "      <td>0.450100</td>\n",
              "      <td>0.256600</td>\n",
              "      <td>0.383100</td>\n",
              "      <td>0.381900</td>\n",
              "      <td>0.258300</td>\n",
              "      <td>[0.495260663507109, 0.3058252427184466, 0.22139303482587064, 0.1683673469387755]</td>\n",
              "      <td>0.942500</td>\n",
              "      <td>0.944100</td>\n",
              "      <td>422</td>\n",
              "      <td>447</td>\n",
              "      <td>44.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.230400</td>\n",
              "      <td>1.208618</td>\n",
              "      <td>0.467200</td>\n",
              "      <td>0.280300</td>\n",
              "      <td>0.396900</td>\n",
              "      <td>0.397700</td>\n",
              "      <td>0.280500</td>\n",
              "      <td>[0.5070422535211268, 0.3245192307692308, 0.23891625615763548, 0.1919191919191919]</td>\n",
              "      <td>0.951900</td>\n",
              "      <td>0.953000</td>\n",
              "      <td>426</td>\n",
              "      <td>447</td>\n",
              "      <td>44.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7d6b405eaf80>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 101, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1e8fdbaa98be>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"tg-summarizer\" # name your W&B project\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_awesome_billsum_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=3,\n",
        "    per_device_eval_batch_size=3,\n",
        "    logging_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=20,\n",
        "    predict_with_generate=True,\n",
        "    # fp16=use_fp16,\n",
        "    # bf16=True,\n",
        "    generation_max_length=100,\n",
        "    # push_to_hub=True,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"].select(range(1000)), # out of 12k\n",
        "    eval_dataset=tokenized_dataset[\"validation\"].select(range(10)),\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnzhWpjPZ9U2"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P32EzyogZ9U2"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEidf6FlZ9U2"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for summarization, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB4mdK-9Z9U2"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 10\n",
        "validation_data = dataset_dict[\"validation\"].select(range(n_samples))"
      ],
      "metadata": {
        "id": "0z11zm711P6g"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = validation_data[0][\"dialogue\"]"
      ],
      "metadata": {
        "id": "sO34tZ483B2c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "68stLuAYZ9U3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e8443a-b2f7-48fe-ae6f-842ebd4677df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': '#–ß–µ–ª–æ–≤–µ–∫2# –∂–∞–ª—É–µ—Ç—Å—è, —á—Ç–æ —É –Ω–µ–≥–æ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥—ã—Ö–∞–Ω–∏–µ–º. #Person1# —Å–æ–æ–±—â–∞–µ—Ç #Person1#, —á—Ç–æ —É #Person1# –Ω–µ—Ç –∞–ª–ª–µ—Ä–≥–∏–∏.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "summarizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log the results to W&B:"
      ],
      "metadata": {
        "id": "u58d8ySn0MGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "4rclymDF0Nto"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = wandb.Table(columns=[\"Input Text\", \"Target Summary\", \"Generated Summary\"])\n",
        "\n",
        "# Process each example in the validation dataset and append to the table_data list\n",
        "for example in validation_data:\n",
        "    input_text = example[\"dialogue\"]  # Replace with the actual key in your dataset\n",
        "    target_summary = example[\"summary\"]  # Replace with the actual key in your dataset\n",
        "\n",
        "    # Generate summary using the pipeline\n",
        "    generated_summary = summarizer(input_text, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Append row to table_data\n",
        "    table.add_data(input_text, target_summary, generated_summary[0][\"summary_text\"])\n",
        "\n",
        "# Create a WandB Table and log it\n",
        "wandb.log({\"summarization_after_fine_tunin\": table})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89NMkT2q1MMQ",
        "outputId": "658b6734-ecec-4ff7-86a4-c3cd60c17ecd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 150, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ddb7567b252d418c84fd6040e7cb25a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ff207ccd9e1484392ce127a63a2b4e7",
              "IPY_MODEL_da66efcd6ed54ad5907c681515a16914",
              "IPY_MODEL_1c52ce0e552242bbaf247ffdb89b4b3d"
            ],
            "layout": "IPY_MODEL_40a40fcfe326431d9d0b9cc5c5965f09"
          }
        },
        "8ff207ccd9e1484392ce127a63a2b4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfda6ef5da74470892a925dce1d89986",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ce3d5bca46a1434d84e3ba0c3e88fc12",
            "value": "Map: 100%"
          }
        },
        "da66efcd6ed54ad5907c681515a16914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2b736b2c997482b86bd92d737a3c424",
            "max": 12460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0f2bd31c0fb441ba36e648107aff3b2",
            "value": 12460
          }
        },
        "1c52ce0e552242bbaf247ffdb89b4b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd753e1076db44528e982f7da838c202",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_608c5998c55748f5b30b4416168ba7b3",
            "value": " 12460/12460 [00:12&lt;00:00, 806.28 examples/s]"
          }
        },
        "40a40fcfe326431d9d0b9cc5c5965f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfda6ef5da74470892a925dce1d89986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3d5bca46a1434d84e3ba0c3e88fc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2b736b2c997482b86bd92d737a3c424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f2bd31c0fb441ba36e648107aff3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd753e1076db44528e982f7da838c202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608c5998c55748f5b30b4416168ba7b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed8ef53e0fac487589268fe5fea00361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_471d53342b1d4001839acb02ec7f2013",
              "IPY_MODEL_ae0570496d8f44fd9ac9aa3a096c1cea",
              "IPY_MODEL_4a43966de75d4473ad3069797473b402"
            ],
            "layout": "IPY_MODEL_c46aba7038db4e17902a7d0891030eb1"
          }
        },
        "471d53342b1d4001839acb02ec7f2013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2c8389e0a60466e96b5f41b608f7399",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6cc7a436bbd64cc48c8953a1f456d3ac",
            "value": "Map: 100%"
          }
        },
        "ae0570496d8f44fd9ac9aa3a096c1cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3156d3d9f58a4f15965855590642be57",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbc54db6f1ab4a0e839bde9c1d5fb5d5",
            "value": 500
          }
        },
        "4a43966de75d4473ad3069797473b402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72974cf3701841df830072f6a57ad4b3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30ad468f29304da59c6131601ead5182",
            "value": " 500/500 [00:00&lt;00:00, 717.82 examples/s]"
          }
        },
        "c46aba7038db4e17902a7d0891030eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c8389e0a60466e96b5f41b608f7399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cc7a436bbd64cc48c8953a1f456d3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3156d3d9f58a4f15965855590642be57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc54db6f1ab4a0e839bde9c1d5fb5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72974cf3701841df830072f6a57ad4b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ad468f29304da59c6131601ead5182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}